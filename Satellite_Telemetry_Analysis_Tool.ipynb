{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d61b78eb",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ›°ï¸ Satellite Telemetry Data Analysis Tool (Jupyter Notebook)\n",
    "\n",
    "**Goal:** Parse, clean, visualize, and generate simple status reports from satellite-like telemetry data.\n",
    "\n",
    "**Dataset options:**  \n",
    "- **Recommended (real telemetry):** *ESA OPS-SAT 1 Re-entry UHF Telemetry* (CSV inside the ZIP). After downloading, set `data_file` to the extracted CSV file path.  \n",
    "- **Quick start (no download):** This notebook includes a fallback **synthetic dataset** at `/mnt/data/telemetry_example.csv`.\n",
    "\n",
    "**What you'll learn/do:**\n",
    "1. Load and inspect telemetry data\n",
    "2. Clean timestamps & handle missing values\n",
    "3. Plot key health parameters (voltage, temperature, current)\n",
    "4. Detect simple anomalies (rule-based + rolling z-score)\n",
    "5. Generate a daily status summary and export reports\n",
    "\n",
    "> Tip: Run each cell from top to bottom. If you donâ€™t have the real dataset yet, the notebook will automatically use the synthetic CSV.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66946ec",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Setup\n",
    "\n",
    "Install these if needed in your own environment (skip here if already available):\n",
    "\n",
    "```bash\n",
    "pip install pandas matplotlib numpy\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7a850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports and display options\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.width\", 120)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8755e4a9",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Choose your data file\n",
    "\n",
    "- If you downloaded **OPS-SAT 1 UHF telemetry** (CSV), set `data_file` to its path.  \n",
    "- Otherwise, leave it as `None` and the notebook will use the built-in synthetic dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cf51ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path to your real telemetry CSV (set this if you downloaded OPS-SAT 1 data)\n",
    "data_file = None  # e.g., data_file = \"/home/you/downloads/uhf_telemetry.csv\"\n",
    "\n",
    "# Fallback synthetic data shipped with this project (works out of the box)\n",
    "fallback_file = \"/mnt/data/telemetry_example.csv\"\n",
    "\n",
    "csv_path = Path(data_file) if data_file else Path(fallback_file)\n",
    "print(f\"Using: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96096000",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Load and preview data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9a6996",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.head())\n",
    "display(df.describe(include='all'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1df476",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Parse timestamps and basic cleaning\n",
    "- Convert timestamp to `datetime`\n",
    "- Drop duplicate rows\n",
    "- Sort by time and set index\n",
    "- Optional: interpolate small gaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61fe0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Try common timestamp column names\n",
    "time_col_candidates = [c for c in df.columns if c.lower() in [\"timestamp\", \"time\", \"datetime\", \"date_time\"]]\n",
    "if not time_col_candidates:\n",
    "    raise ValueError(\"No obvious timestamp column found. Please rename your time column to 'timestamp'.\")\n",
    "\n",
    "time_col = time_col_candidates[0]\n",
    "df[time_col] = pd.to_datetime(df[time_col], errors='coerce', utc=True)\n",
    "\n",
    "# Drop bad timestamps\n",
    "df = df.dropna(subset=[time_col]).copy()\n",
    "df = df.drop_duplicates().sort_values(time_col).reset_index(drop=True)\n",
    "\n",
    "# Set index\n",
    "df = df.set_index(time_col)\n",
    "\n",
    "# Basic numeric columns (auto-detect common telemetry names)\n",
    "def pick(colset):\n",
    "    cols = []\n",
    "    for name in colset:\n",
    "        cols.extend([c for c in df.columns if name in c.lower()])\n",
    "    return list(dict.fromkeys(cols))\n",
    "\n",
    "voltage_cols = pick([\"volt\"])\n",
    "temp_cols    = pick([\"temp\"])\n",
    "curr_cols    = pick([\"curr\"])\n",
    "rssi_cols    = pick([\"rssi\"])\n",
    "\n",
    "numeric_cols = list(dict.fromkeys([*voltage_cols, *temp_cols, *curr_cols, *rssi_cols]))\n",
    "numeric_cols = [c for c in numeric_cols if pd.api.types.is_numeric_dtype(df[c])]\n",
    "\n",
    "print(\"Detected numeric telemetry columns:\", numeric_cols)\n",
    "\n",
    "# Interpolate tiny gaps (optional)\n",
    "df[numeric_cols] = df[numeric_cols].interpolate(limit=5).bfill().ffill()\n",
    "\n",
    "# Resample to a uniform timeline (e.g., 1 minute) if data is high-rate\n",
    "df_res = df.copy()\n",
    "if df.index.inferred_type != \"datetime64\":\n",
    "    raise TypeError(\"Index must be datetime after parsing.\")\n",
    "\n",
    "# Use 1-minute resample only if density is high; otherwise keep original\n",
    "if df.index.to_series().diff().median() < pd.Timedelta(\"30s\"):\n",
    "    df_res = df[numeric_cols].resample(\"1min\").mean().join(df.drop(columns=numeric_cols).resample(\"1min\").first())\n",
    "else:\n",
    "    df_res = df\n",
    "\n",
    "print(\"Final shape after cleaning/resampling:\", df_res.shape)\n",
    "display(df_res.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9700743",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Plot key parameters (one chart per variable)\n",
    "> Note: We avoid setting custom colors and only use Matplotlib, as requested.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26a74b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_series(series, title):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    series.plot()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Time (UTC)\")\n",
    "    plt.ylabel(series.name)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for col in numeric_cols[:6]:  # keep it reasonable\n",
    "    plot_series(df_res[col].dropna(), f\"{col} over time\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8e1eac",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Simple anomaly detection\n",
    "Two approaches:\n",
    "1. **Rule-based thresholds** (set plausible min/max per signal)  \n",
    "2. **Rolling z-score** (flag points deviating strongly from local mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666a5d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5.1 Rule-based thresholds (customize as needed)\n",
    "thresholds = {}\n",
    "for c in numeric_cols:\n",
    "    s = df_res[c].dropna()\n",
    "    if s.empty:\n",
    "        continue\n",
    "    lo, hi = s.quantile(0.01), s.quantile(0.99)  # heuristic start\n",
    "    # widen a bit\n",
    "    pad = 0.1 * (hi - lo) if np.isfinite(hi - lo) else 0\n",
    "    thresholds[c] = (float(lo - pad), float(hi + pad))\n",
    "\n",
    "thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aaf263",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rule_based_flags(df_num, thresholds):\n",
    "    flags = {}\n",
    "    for c, (lo, hi) in thresholds.items():\n",
    "        s = df_num[c]\n",
    "        flags[c] = (s < lo) | (s > hi)\n",
    "    return pd.DataFrame(flags, index=df_num.index)\n",
    "\n",
    "rb_flags = rule_based_flags(df_res[numeric_cols], thresholds)\n",
    "rb_counts = rb_flags.sum().sort_values(ascending=False)\n",
    "print(\"Rule-based anomaly counts per signal:\")\n",
    "display(rb_counts.to_frame(\"count\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb8f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5.2 Rolling z-score\n",
    "def rolling_z_flags(df_num, window=30, z=3.0):\n",
    "    flags = {}\n",
    "    for c in df_num.columns:\n",
    "        s = df_num[c].astype(float)\n",
    "        mu = s.rolling(window, min_periods=max(5, window//3)).mean()\n",
    "        sd = s.rolling(window, min_periods=max(5, window//3)).std(ddof=0)\n",
    "        zsc = (s - mu) / sd\n",
    "        flags[c] = zsc.abs() > z\n",
    "    return pd.DataFrame(flags, index=df_num.index)\n",
    "\n",
    "z_flags = rolling_z_flags(df_res[numeric_cols], window=30, z=3.0)\n",
    "z_counts = z_flags.sum().sort_values(ascending=False)\n",
    "print(\"Rolling-z anomaly counts per signal:\")\n",
    "display(z_counts.to_frame(\"count\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cdae01",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Daily status summary\n",
    "Aggregate min/max/mean and anomaly counts per day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6a8c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "daily_stats = df_res[numeric_cols].resample(\"1D\").agg([\"min\",\"mean\",\"max\"])\n",
    "daily_rb = rb_flags.resample(\"1D\").sum().add_suffix(\"_rbFlags\")\n",
    "daily_z  = z_flags.resample(\"1D\").sum().add_suffix(\"_zFlags\")\n",
    "\n",
    "daily = daily_stats.join(daily_rb).join(daily_z)\n",
    "display(daily.tail())\n",
    "\n",
    "# Save report files\n",
    "out_dir = Path(\"/mnt/data\")\n",
    "(out_dir / \"status_reports\").mkdir(parents=True, exist_ok=True)\n",
    "daily.to_csv(out_dir / \"status_reports\" / \"daily_status_summary.csv\")\n",
    "\n",
    "print(\"Saved:\", out_dir / \"status_reports\" / \"daily_status_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f570f4",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Export cleaned data and a short text report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35edf3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clean_path = Path(\"/mnt/data/cleaned_telemetry.csv\")\n",
    "df_res.to_csv(clean_path)\n",
    "print(\"Cleaned data saved to:\", clean_path)\n",
    "\n",
    "# Create a tiny text summary\n",
    "top_rb = rb_counts.head(3).to_string()\n",
    "top_z  = z_counts.head(3).to_string()\n",
    "\n",
    "report = f\"\"\"Satellite Telemetry Status Report\n",
    "===============================\n",
    "Rows (cleaned): {len(df_res):,}\n",
    "Time span: {df_res.index.min()}  â†’  {df_res.index.max()}\n",
    "\n",
    "Top rule-based anomalies:\n",
    "{top_rb}\n",
    "\n",
    "Top rolling-z anomalies:\n",
    "{top_z}\n",
    "\n",
    "Notes:\n",
    "- Thresholds were initialized from the 1st and 99th percentiles (then padded).\n",
    "- Rolling window = 30 samples for z-score (adjust if your sampling period differs).\n",
    "- Inspect plots above for context around flagged points.\n",
    "\"\"\"\n",
    "\n",
    "txt_path = Path(\"/mnt/data/status_reports/summary.txt\")\n",
    "txt_path.write_text(report, encoding=\"utf-8\")\n",
    "print(\"Text report saved to:\", txt_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7262d3e",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Next steps\n",
    "- Tune thresholds per signal with domain knowledge\n",
    "- Add per-mode analysis (e.g., split by ADCS mode if available)\n",
    "- Create a Streamlit or PyQt dashboard\n",
    "- Automate daily report generation (cron, Airflow, or simple script)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
